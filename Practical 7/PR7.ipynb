{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fbea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12377782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid activation function and its derivative for backpropagation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a59b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input datasets\n",
    "inputs = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]])\n",
    "\n",
    "expected_output = np.array([[0],\n",
    "                            [1],\n",
    "                            [1],\n",
    "                            [0]])\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2, 2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f1cc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons, hiddenLayerNeurons))\n",
    "hidden_bias = np.random.uniform(size=(1, hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons, outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1, outputLayerNeurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af5a7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training algorithm\n",
    "for _ in range(epochs):\n",
    "    # Forward Propagation\n",
    "    hidden_layer_activation = np.dot(inputs, hidden_weights)\n",
    "    hidden_layer_activation += hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output, output_weights)\n",
    "    output_layer_activation += output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    # Backpropagation\n",
    "    error = expected_output - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Updating Weights and Biases\n",
    "    output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
    "    hidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35716bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output from neural network after 10,000 epochs: \n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final output from training\n",
    "print(\"Predicted output from neural network after 10,000 epochs: \")\n",
    "for i in predicted_output:\n",
    "    for j in i:\n",
    "        if j>=0.5:\n",
    "            j=1\n",
    "        else:\n",
    "            j=0\n",
    "        print(j)\n",
    "# print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "\n",
    "# # Sigmoid activation function and its derivative for backpropagation\n",
    "# def sigmoid(x):\n",
    "#     \"\"\"The sigmoid activation function that maps any real value into the (0, 1) interval.\"\"\"\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     \"\"\"The derivative of the sigmoid function used to compute gradients during backpropagation.\"\"\"\n",
    "#     return x * (1 - x)  \n",
    "\n",
    "# # Input datasets\n",
    "# inputs = np.array([[0, 0],\n",
    "#                    [0, 1],\n",
    "#                    [1, 0],\n",
    "#                    [1, 1]])\n",
    "\n",
    "# expected_output = np.array([[0],\n",
    "#                             [1],\n",
    "#                             [1],\n",
    "#                             [0]])\n",
    "\n",
    "# # Define parameters for the model\n",
    "# epochs = 10000  # Number of cycles through the full dataset\n",
    "# lr = 0.1        # Learning rate\n",
    "# inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2, 2, 1 \n",
    "\n",
    "# # Random weights and bias initialization for both hidden and output layers\n",
    "# hidden_weights = np.random.uniform(size=(inputLayerNeurons, hiddenLayerNeurons))\n",
    "# hidden_bias = np.random.uniform(size=(1, hiddenLayerNeurons))\n",
    "# output_weights = np.random.uniform(size=(hiddenLayerNeurons, outputLayerNeurons))\n",
    "# output_bias = np.random.uniform(size=(1, outputLayerNeurons))\n",
    "\n",
    "# # Training algorithm\n",
    "# for _ in range(epochs):\n",
    "#     # Forward Propagation\n",
    "#     hidden_layer_activation = np.dot(inputs, hidden_weights)\n",
    "#     hidden_layer_activation += hidden_bias\n",
    "#     hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "#     output_layer_activation = np.dot(hidden_layer_output, output_weights)\n",
    "#     output_layer_activation += output_bias\n",
    "#     predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "#     # Backpropagation\n",
    "#     error = expected_output - predicted_output\n",
    "#     d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "#     error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "#     d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "#     # Updating Weights and Biases\n",
    "#     output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "#     output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
    "#     hidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "#     hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr \n",
    "\n",
    "# # Final output from training\n",
    "# print(\"Predicted output from neural network after 10,000 epochs: \")\n",
    "# for i in predicted_output:\n",
    "#     # Convert probabilities to binary output 0 or 1\n",
    "#     for j in i:\n",
    "#         if j >= 0.5:\n",
    "#             j = 1\n",
    "#         else:\n",
    "#             j = 0\n",
    "#         print(j)\n",
    "# # Uncomment below line to see raw sigmoid outputs from the network\n",
    "# # print(predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb9653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ef747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
